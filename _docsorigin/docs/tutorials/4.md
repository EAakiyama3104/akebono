# BigQuery loaderを使う

## SQLを直接書く

まず、最も単純な設定から始めます。データとしては、公開データセットのテーブルである`gdelt-bq.hathitrustbooks.1800`を使います。
また、目的変数としては便宜的に`DATE`を使います。

```python
In [1]: from akebono.dataset import get_dataset

In [2]: ds = get_dataset({ 
   ...:     'loader_config': {
   ...:         'name': 'bigquery',
   ...:         'param': {
   ...:             'sql': 'select DATE,SourceCollectionIdentifier,SourceCommonName,DocumentIdentifier from `gdelt-bq.hathitrustbooks.1800`',
   ...:         }, 
   ...:     }, 
   ...:     'target_column': 'DATE'
   ...: })

In [3]: ds.value.head()
Out[3]: 
   DATE DocumentIdentifier  SourceCollectionIdentifier SourceCommonName
0  1800     wu.89098017890                           8  HathiTrustBooks
1  1800     wu.89097348304                           8  HathiTrustBooks
2  1800     wu.89094692548                           8  HathiTrustBooks
3  1800     wu.89040484586                           8  HathiTrustBooks
4  1800     wu.89094364684                           8  HathiTrustBooks
```

ここでは、SQLを発行して返ってきたデータをpandas.DataFrame化しています。

## データセットに名前をつけてキャッシュを有効にする

次は、データセットに名前をつけてキャッシュを有効にしてみます。
`name`を`hathitrustbooks_1800`に、`cache_enabled`を`True`にします。

```python
In [1]: from akebono.dataset import get_dataset

In [2]: ds2 = get_dataset({ 
   ...:     'name': 'hathitrustbooks_1800',
   ...:     'loader_config': {
   ...:         'name': 'bigquery',
   ...:         'param': {
   ...:             'sql': 'select DATE,SourceCollectionIdentifier,SourceCommonName,DocumentIdentifier from `gdelt-bq.hathitrustbooks.1800`',
   ...:         }, 
   ...:     }, 
   ...:     'target_column': 'DATE',
   ...:     'cache_enabled': True,
   ...: })
[INFO] 2018-08-19 23:54:10,799 dataset cache enabled akebono.dataset.entry

In [3]: ds2.value.head()
Out[3]: 
   DATE DocumentIdentifier  SourceCollectionIdentifier SourceCommonName
0  1800     wu.89098017890                           8  HathiTrustBooks
1  1800     wu.89097348304                           8  HathiTrustBooks
2  1800     wu.89094692548                           8  HathiTrustBooks
3  1800     wu.89040484586                           8  HathiTrustBooks
4  1800     wu.89094364684                           8  HathiTrustBooks
```

ds2は最初と同じ値が格納されています。ここで、キャッシュが作成されていることを確認します。

```
$ ls -l _storage/default/cache/hathitrustbooks_1800_e3b0c44298fc1c149afbf4c8_055aa78309b36ea9aeaf1e89_e3b0c44298fc1c149afbf4c8.pkl | awk '{print $9}'
_storage/default/cache/hathitrustbooks_1800_e3b0c44298fc1c149afbf4c8_055aa78309b36ea9aeaf1e89_e3b0c44298fc1c149afbf4c8.pkl
```

`_storage/default/cache/`に想定通り格納されていました。


## ファイルにSQLを書く

SQLが複雑になってくるとファイルに保存しておきたくなってくるはずで、BigQuery loaderはそのようなユースケースにも対応しています。
デフォルトでは、`_dataset/bq_sql_templates` に`<name>.sql`のファイル名で置きます。
今回は、`name`を`hathitrustbooks_1800_2`とします。
また、SQLファイルはテンプレートとして記述することもできて、`DATE`以外のカラム選択を`config.py`側でできるようにします。

準備するファイルは以下のようになります。ファイル名とパスは`_dataset/bq_sql_templates/hathitrustbooks_1800_2.sql` です。

```
select DATE,{{ columns }} from `gdelt-bq.hathitrustbooks.1800`
```

実行すると以下のようになります。

```python
In [1]: from akebono.dataset import get_dataset

In [2]: ds3 = get_dataset({ 
   ...:     'name': 'hathitrustbooks_1800_2',
   ...:     'loader_config': {
   ...:         'name': 'bigquery',
   ...:         'kwargs': {
   ...:             'columns': 'SourceCollectionIdentifier,SourceCommonName,DocumentIdentifier'
   ...:         }
   ...:     }, 
   ...:     'target_column': 'DATE',
   ...:     'cache_enabled': True,
   ...: })
[INFO] 2018-08-20 00:08:26,784 dataset cache enabled akebono.dataset.entry

In [3]: ds3.value.head()
Out[3]: 
   DATE DocumentIdentifier  SourceCollectionIdentifier SourceCommonName
0  1800     wu.89098017890                           8  HathiTrustBooks
1  1800     wu.89097348304                           8  HathiTrustBooks
2  1800     wu.89094692548                           8  HathiTrustBooks
3  1800     wu.89040484586                           8  HathiTrustBooks
4  1800     wu.89094364684                           8  HathiTrustBooks
```

カラム選択をパラメータ化したので、今度は`SourceCommonName,DocumentIdentifier`の２つのみを選択するように設定して実行してみます。

```python
In [1]: from akebono.dataset import get_dataset

In [2]: ds4 = get_dataset({ 
   ...:     'name': 'hathitrustbooks_1800_2',
   ...:     'loader_config': {
   ...:         'name': 'bigquery',
   ...:         'kwargs': {
   ...:             'columns': 'SourceCommonName,DocumentIdentifier'
   ...:         }
   ...:     }, 
   ...:     'target_column': 'DATE',
   ...:     'cache_enabled': True,
   ...: })
[INFO] 2018-08-20 00:11:03,253 dataset cache enabled akebono.dataset.entry

In [3]: ds4.value.head()
Out[3]: 
   DATE        DocumentIdentifier SourceCommonName
0  1800  uma.ark:/13960/t4cn7hk1t  HathiTrustBooks
1  1800  uma.ark:/13960/t77s7xq1j  HathiTrustBooks
2  1800            wu.89067359901  HathiTrustBooks
3  1800            wu.89058531922  HathiTrustBooks
4  1800            wu.89087923439  HathiTrustBooks
```

期待したとおりのカラムが選択されています。

また、キャッシュは名前とパラメータの組み合わせで生成されるので、今回の例のように名前が同じでもパラメータが異なると別々にキャッシュが生成されます。
実際、生成されたキャッシュは以下のようになっています。

```
$ ls -l _storage/default/cache | awk '{print $9}' | grep hathitrustbooks_1800_2
hathitrustbooks_1800_2_cc5cf0a617cc14dda7359ba7_055aa78309b36ea9aeaf1e89_e3b0c44298fc1c149afbf4c8.pkl
hathitrustbooks_1800_2_f9a51ff16f984c5bda791e0b_055aa78309b36ea9aeaf1e89_e3b0c44298fc1c149afbf4c8.pkl
```
